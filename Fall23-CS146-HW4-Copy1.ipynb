{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WLVhlv99E-h"
   },
   "source": [
    "# Problem 1: A Two-Layer Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jqQZ3TWY9E-i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load matplotlib images inline\n",
    "%matplotlib inline\n",
    "# These are important for reloading any code you write in external .py files.\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MRqR8CZ39E-j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (50000, 576)\n",
      "Train target shape:  (50000,)\n",
      "Val data shape:  (10000, 576)\n",
      "Val target shape:  (10000,)\n",
      "Test data shape:  (10000, 576)\n",
      "Test target shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "## Load fashionMNIST. This is the same code with homework 1.\n",
    "##\n",
    "def crop_center(img,cropped):\n",
    "    img = img.reshape(-1, 28, 28)\n",
    "    start = 28//2-(cropped//2)\n",
    "    img = img[:, start:start+cropped, start:start+cropped]\n",
    "    return img.reshape(-1, cropped*cropped)\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,'%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz'% kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), 'B', offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(),'B', offset=16).reshape(-1, 784)\n",
    "        images = crop_center(images, 24)\n",
    "    return images, labels\n",
    "X_train_and_val, y_train_and_val = load_mnist('./data/mnist', kind='train')\n",
    "X_test, y_test = load_mnist('./data/mnist', kind='t10k')\n",
    "X_train, X_val = X_train_and_val[:50000], X_train_and_val[50000:]\n",
    "y_train, y_val = y_train_and_val[:50000], y_train_and_val[50000:]\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target shape: ', y_train.shape)\n",
    "print('Val data shape: ', X_val.shape)\n",
    "print('Val target shape: ', y_val.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test target shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xjfsY8Ox9E-j"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label is Odd\n"
     ]
    }
   ],
   "source": [
    "# PART (a):\n",
    "# To Visualize a point in the dataset\n",
    "index = 10\n",
    "X = np.array(X_train[index], dtype='uint8').reshape([24, 24])\n",
    "fig = plt.figure()\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.show()\n",
    "if y_train[index] in set([1, 3, 5, 7, 9]):\n",
    "    label = 'Odd'\n",
    "else:\n",
    "    label = 'Even'\n",
    "print('Label is', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ozhJL4T9E-j"
   },
   "source": [
    "In the following cells, you will build a two-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mo9zmasa9E-j"
   },
   "outputs": [],
   "source": [
    "# convert to binary label\n",
    "y_train = y_train.astype(int) % 2\n",
    "y_val = y_val.astype(int) % 2\n",
    "y_test = y_test.astype(int) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WzU3563s9E-j"
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network for binary classification.\n",
    "    We train the network with a softmax output and cross entropy loss function\n",
    "    with L2 regularization on the weight matrices. The network uses a ReLU\n",
    "    nonlinearity after the first fully connected layer.\n",
    "    Input: X\n",
    "    Hidden states for layer 1: h1 = XW1 + b1\n",
    "    Activations: a1 = ReLU(h1)\n",
    "    Hidden states for layer 2: h2 = a1W2 + b2\n",
    "    Probabilities: s = softmax(h2)\n",
    "\n",
    "    ReLU function:\n",
    "    (i) x = x if x >= 0  (ii) x = 0 if x < 0\n",
    "\n",
    "    The outputs of the second fully-connected layer are the scores for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize the model. Weights are initialized to small random values and\n",
    "        biases are initialized to zero. Weights and biases are stored in the\n",
    "        variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "        W1: First layer weights; has shape (D, H)\n",
    "        b1: First layer biases; has shape (H,)\n",
    "        W2: Second layer weights; has shape (H, C)\n",
    "        b2: Second layer biases; has shape (C,)\n",
    "\n",
    "        Inputs:\n",
    "        - input_size: The dimension D of the input data.\n",
    "        - hidden_size: The number of neurons H in the hidden layer.\n",
    "        - output_size: The number of classes C.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a two layer fully connected neural\n",
    "        network.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "          an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "          is not passed then we only return scores, and if it is passed then we\n",
    "          instead return the loss and gradients.\n",
    "        - reg: Regularization strength.\n",
    "\n",
    "        Returns:\n",
    "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "        the score for class c on input X[i].\n",
    "\n",
    "        If y is not None, instead return a tuple of:\n",
    "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "          samples.\n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "          with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Compute the forward pass\n",
    "\n",
    "        ### ========== TODO : START ========== ###\n",
    "        #   Calculate the output of the neural network using forward pass.\n",
    "        #   The expected result should be a matrix of shape (N, C), where:\n",
    "        #     - N is the number of examples in the input dataset 'X'.\n",
    "        #     - C is the number of classes.\n",
    "        #   Use 'h1' as the first hidden layer output\n",
    "        h1=np.dot(X, self.params['W1']) + self.params['b1']\n",
    "        #   Apply the ReLU activation function to 'h1' to get 'a1'. Use np.maximum for ReLU implementation.\n",
    "        a1 = np.maximum(0, h1)\n",
    "        #   The output 'scores' is the result of the second layer (before applying softmax).\n",
    "        scores = np.dot(a1, self.params['W2']) + self.params['b2']\n",
    "        #   Refer to the model architecture comments at the beginning of this class for more details.\n",
    "        #   Note: Do not use a for loop in your implementation.\n",
    "        ##  Part (b): Implement the forward pass and compute scores.\n",
    "        \n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "        # If the targets are not given then jump out, we're done\n",
    "        if y is None:\n",
    "            return scores\n",
    "        \n",
    "        if y.ndim == 1:\n",
    "            y_one_hot = np.zeros((N, W2.shape[1]))\n",
    "            y_one_hot[np.arange(N), y] = 1\n",
    "        else:\n",
    "            y_one_hot = y\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = None\n",
    "\n",
    "        # scores is num_examples by num_classes (N, C)\n",
    "        def softmax_loss(x, y):\n",
    "            ### ========== TODO : START ========== ###\n",
    "            #   Calculate the cross entropy loss after softmax output layer.\n",
    "            #   This function should return loss and dx\n",
    "            #probs = np.exp(x - np.max(x, axis=1, keepdims=True)) # Other Notes: \n",
    "            #this operation is called stable softmax: numerically more stable as it reduces \n",
    "            #overflow issues by not letting the numerator and denominator grow too big.\n",
    "            #probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "            \n",
    "            \n",
    "            shifted_logits = x - np.max(x, axis=1, keepdims=True)\n",
    "            Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "            log_probs = shifted_logits - np.log(Z)\n",
    "            probs = np.exp(log_probs)\n",
    "            \n",
    "            N = x.shape[0]\n",
    "            ##  Part (d): Implement the CrossEntropyLoss\n",
    "            loss = -np.sum(y * log_probs) / N\n",
    "            ##  Part (d): Implement the gradient of y wrt x\n",
    "            dx = (probs - y) / N\n",
    "\n",
    "            ### ========== TODO : END ========== ###\n",
    "            return loss, dx\n",
    "\n",
    "        \n",
    "        data_loss, dscore = softmax_loss(scores, y_one_hot)\n",
    "        \n",
    "\n",
    "        ### ========== TODO : START ========== ###\n",
    "        #   Calculate the regularization loss. Multiply the regularization\n",
    "        #   loss by 0.5 (in addition to the regularization factor 'reg').\n",
    "        ##  Part (c): Implement the regularization loss\n",
    "        reg_loss = 0.5 * reg * (np.sum(W1 * W1) + np.sum(W2 * W2))\n",
    "        \n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "        loss = data_loss + reg_loss\n",
    "\n",
    "\n",
    "        grads = {}\n",
    "\n",
    "        ### ========== TODO : START ========== ###\n",
    "        #  Compute backpropagation\n",
    "        #  Remember the loss contains two parts: cross-entropy and regularization. \n",
    "        # The computation for gradients of W1 and b1 shown here can be regarded as a reference.\n",
    "        ## Part (e): Implement the computations of gradients for W2 and b2.\n",
    "        grads['W2'] = np.dot(a1.T, dscore) + reg * W2\n",
    "        grads['b2'] = np.sum(dscore, axis=0) \n",
    "\n",
    "        dh = np.dot(dscore, W2.T)\n",
    "        dh[a1 <= 0] = 0\n",
    "\n",
    "        grads['W1'] = np.dot(X.T, dh) + reg * W1\n",
    "        #grads['b1'] = np.ones(N).dot(dh)\n",
    "        grads['b1'] = np.sum(dh, axis=0) \n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this neural network using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving training data.\n",
    "        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
    "          X[i] has label c, where 0 <= c < C.\n",
    "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "        - learning_rate: Scalar giving learning rate for optimization.\n",
    "        - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "          after each epoch.\n",
    "        - reg: Scalar giving regularization strength.\n",
    "        - num_iters: Number of steps to take when optimizing.\n",
    "        - batch_size: Number of training examples to use per step.\n",
    "        - verbose: boolean; if true print progress during optimization.\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        for it in np.arange(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            #   Create a minibatch (X_batch, y_batch) by sampling batch_size\n",
    "            #   samples randomly.\n",
    "\n",
    "            b_index = np.random.choice(num_train, batch_size)\n",
    "            X_batch = X[b_index]\n",
    "            y_batch = y[b_index]\n",
    "\n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "\n",
    "            self.params['W1'] -= learning_rate * grads['W1']\n",
    "            self.params['b1'] -= learning_rate * grads['b1']\n",
    "            self.params['W2'] -= learning_rate * grads['W2']\n",
    "            self.params['b2'] -= learning_rate * grads['b2']\n",
    "\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration {} / {}: loss {}'.format(it, num_iters, loss))\n",
    "\n",
    "            # Every epoch, check train and val accuracy and decay learning rate.\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # Check accuracy\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "                # Decay learning rate\n",
    "                learning_rate *= learning_rate_decay\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'train_acc_history': train_acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this two-layer network to predict labels for\n",
    "        data points. For each data point we predict scores for each of the C\n",
    "        classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "        Inputs:\n",
    "        \n",
    "        \n",
    "        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "          classify.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "          to have class c, where 0 <= c < C.\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "\n",
    "        ### ========== TODO : START ========== ###\n",
    "        #   Predict the class given the input data.\n",
    "        ##  Part (f): Implement the prediction function\n",
    "        h = np.maximum(0, np.dot(X, self.params['W1']) + self.params['b1'])\n",
    "        scores = np.dot(h, self.params['W2']) + self.params['b2']\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "\n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O1NPs0Jy9E-k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate:  1e-05\n",
      "iteration 0 / 1000: loss 0.6931612201514842\n",
      "iteration 100 / 1000: loss 0.6931458855346444\n",
      "iteration 200 / 1000: loss 0.6931212971576897\n",
      "iteration 300 / 1000: loss 0.6930952908562985\n",
      "iteration 400 / 1000: loss 0.6930526095224442\n",
      "iteration 500 / 1000: loss 0.6929951796505565\n",
      "iteration 600 / 1000: loss 0.6929417822800262\n",
      "iteration 700 / 1000: loss 0.6928767315148259\n",
      "iteration 800 / 1000: loss 0.692735299875578\n",
      "iteration 900 / 1000: loss 0.6924231541807353\n",
      "Validation accuracy:  0.7168\n",
      "Test accuracy (subopt_net):  0.7193\n",
      "\n",
      "\n",
      "learning_rate:  0.0001\n",
      "iteration 0 / 1000: loss 0.6924146308208956\n",
      "iteration 100 / 1000: loss 0.6591153875714295\n",
      "iteration 200 / 1000: loss 0.46075401299809776\n",
      "iteration 300 / 1000: loss 0.3729930470715791\n",
      "iteration 400 / 1000: loss 0.3378848663881613\n",
      "iteration 500 / 1000: loss 0.3314818059099768\n",
      "iteration 600 / 1000: loss 0.38090188791293794\n",
      "iteration 700 / 1000: loss 0.27570760683660667\n",
      "iteration 800 / 1000: loss 0.32576526488664365\n",
      "iteration 900 / 1000: loss 0.2870450385795343\n",
      "Validation accuracy:  0.8846\n",
      "Test accuracy (subopt_net):  0.8802\n",
      "\n",
      "\n",
      "learning_rate:  0.001\n",
      "iteration 0 / 1000: loss 0.34873851475537104\n",
      "iteration 100 / 1000: loss 0.2847195317327137\n",
      "iteration 200 / 1000: loss 0.2662448335409426\n",
      "iteration 300 / 1000: loss 0.23288541111253053\n",
      "iteration 400 / 1000: loss 0.1483882370937818\n",
      "iteration 500 / 1000: loss 0.09933217934088122\n",
      "iteration 600 / 1000: loss 0.12670178642315363\n",
      "iteration 700 / 1000: loss 0.07881572883326705\n",
      "iteration 800 / 1000: loss 0.060084111681206656\n",
      "iteration 900 / 1000: loss 0.08663753309628731\n",
      "Validation accuracy:  0.9695\n",
      "Test accuracy (subopt_net):  0.9663\n",
      "\n",
      "\n",
      "learning_rate:  0.005\n",
      "iteration 0 / 1000: loss 0.06091326507518592\n",
      "iteration 100 / 1000: loss 0.7522409939617848\n",
      "iteration 200 / 1000: loss 0.7488578789907545\n",
      "iteration 300 / 1000: loss 0.744711871089461\n",
      "iteration 400 / 1000: loss 0.7398931738859789\n",
      "iteration 500 / 1000: loss 0.7357220360473122\n",
      "iteration 600 / 1000: loss 0.7314613661149956\n",
      "iteration 700 / 1000: loss 0.6872293567788631\n",
      "iteration 800 / 1000: loss 0.5230960776083291\n",
      "iteration 900 / 1000: loss 0.4354398951321166\n",
      "Validation accuracy:  0.6835\n",
      "Test accuracy (subopt_net):  0.6765\n",
      "\n",
      "\n",
      "learning_rate:  0.1\n",
      "iteration 0 / 1000: loss 0.6388422024954507\n",
      "iteration 100 / 1000: loss 17.765148760669632\n",
      "iteration 200 / 1000: loss 3.223660195621109\n",
      "iteration 300 / 1000: loss 1.0861088539057075\n",
      "iteration 400 / 1000: loss 0.7590729384177753\n",
      "iteration 500 / 1000: loss 0.7056981722091212\n",
      "iteration 600 / 1000: loss 0.6951372370849859\n",
      "iteration 700 / 1000: loss 0.6926268497783978\n",
      "iteration 800 / 1000: loss 0.6943520400726565\n",
      "iteration 900 / 1000: loss 0.6931093314050666\n",
      "Validation accuracy:  0.506\n",
      "Test accuracy (subopt_net):  0.5074\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = 576\n",
    "hidden_size = 50\n",
    "num_classes = 2\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "for learning_rate in [1e-5, 1e-4, 1e-3, 5e-3, 1e-1]:\n",
    "    print('learning_rate: ', learning_rate)\n",
    "    stats = net.train(X_train, y_train, X_val, y_val,\n",
    "              num_iters=1000, batch_size=200,\n",
    "              learning_rate=learning_rate, learning_rate_decay=0.95,\n",
    "              reg=0.1, verbose=True)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    val_acc = (net.predict(X_val) == y_val).mean()\n",
    "    print('Validation accuracy: ', val_acc)\n",
    "\n",
    "    # Save this net as the variable subopt_net for later comparison.\n",
    "    subopt_net = net\n",
    "    test_acc = (subopt_net.predict(X_test) == y_test).mean()\n",
    "    print('Test accuracy (subopt_net): ', test_acc)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
